<script type="text/javascript">
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      renderActions: {
        findScript: [10, function (doc) {
          // escape Markdown italics (*) inside math
          for (const script of document.querySelectorAll('script[type^="math/tex"]')) {
            script.text = script.text.replace(/\^([^\s^_{}\\])/g, '^{$1}');
          }
        }, '']
      }
    }
  };
</script>

<script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


# ðŸŽ¯ Policy Gradient: *Lâ€™Equazione Elegante*

In this blog, I aim to explore the elegant derivation of the policy gradient. Why? Because I find the equation not just insightful, but genuinely beautiful in its simplicity and structure. And Iâ€™d like to share that appreciation with anyone who reads this.

So, the goal here is to explain the derivation in a way thatâ€™s digestible for a novice reader. That said, there might be parts where I donâ€™t explain things well enough â€” and that could interrupt the flow a bit. For that, I ask a little patience (and effort) from you, the reader. Honestly, this is also my way of pushing myself to understand things better to the point where I can explain them without using fancy terminology at all. But hey, this is just *iteration one*, and Iâ€™ll keep improving it as I go. So, pardon me â€” and thanks in advance for bearing with me. 

>These notes are based from the lectures of Sergey Levine [^1], Lilian Wang blog and the Reinforcement Learning book by Sutton and Barto

---

So first of all what's a policy ? A policy $\pi$ is simply an agent's way of behaving in an environment. More specifically, if we consider $A$ = {$a_1, \cdots, a_k$ } to be the set of actions applicable in a particular state $s_t$ of an environment (at timestep $t$), then the policy specifies a probability distribution over these $k$ actions. 

$$
\pi(a_t=a_i|s_t)=P(a_i|s_t) \ \ \ \ i\in\{1, \cdots, k\}
$$

Now, the question is how do we find that optimal policy ? More specifically what makes a policy optimal ? These are some of the questions we need to answer. 

Since we are introducing terms in the line of optimality, we need to first quantify the poilicies. A viable method would be to assess the performance of a policy by finding out the expected return obtained from behaving according to the policy in a particular environment. In other words, the performance of a policy $\eta(\pi)$ can be assessed by running the policy multiple times (ideally infinite) and summing the rewards obtained at each time step $r(s_t, a_t)$ until termination. 

Let $\tau = (s_1, a_1, \cdots, s_T, a_T)$ be a sample trajectory generated using $\pi$. Thus the probability of this particular trajectory being generated by $\pi$ is 

$$
\rho_\pi(\tau) = P(s_1)\prod_{t=1}^{T} \pi(a_t \mid s_t) \cdot P(s_{t+1} \mid s_t, a_t)
$$
> Note : The environment is markov i.e $P(s_{t+1}\mid s_t, a_t)=P(s_{t+1}\mid s_0, a_0, \cdots, s_t, a_t)$

Therefore, $\eta(\pi)$ can be formulated as :
$$
\eta(\pi) = \mathbb{E}_{\tau \sim \rho_\pi(\tau)}\left[\sum_{t=1}^Tr(s_t, a_t)\right]
$$
> Note : Generally we discount the rewards in the form of $\sum_{t=1}^T\gamma^{t-1} r(s_t, a_t)$, and here we have kept $\gamma=1$

With this, we now have a method to quantify the performance of a policy $\pi$. Now what makes a policy optimal? If $\Pi$={ $\pi_1, \pi_2, \cdots, \pi_\infty $ } is the space of all policies, the optimal policy $\pi^\ast$ is the one for which the expected return is the highest. In other words,

$$
\pi^\ast = \arg \max_{\pi\sim \Pi} \eta(\pi)
$$

But $\Pi$ is continous and needless to point out that noones's willing to check each policy out one by one until the end of time when still infinitely more  policies will be left unchecked. So, what do we do ? Well we can parametrize the policy with $\theta$ i.e we can model the policy as a neural network $f$ the parameters of which are $\theta$ and then use *Gradient Ascent* to update the parameters. Also now our search space is no more $\Pi$, as some policies might never get modelled by the network. Therefore our search space is now $\tilde{\Pi}\subseteq \Pi$ where $\tilde{\Pi}$ = { $\pi_\theta\mid \pi_\theta = f(\theta) , \ \ \ \pi_\theta \in \Pi, \ \ \ \forall \theta \in \Theta $ }. By reducing the search space, we can't gurantee global optimality anymore, however its better than waiting till the end of time and that too for solving one environment. Also, our new modified objective is formulated as :

$$
\theta^* = \arg\max_{\theta \in \Theta} \ \mathbb{E}_{\tau \sim \rho_\theta(\tau)} \left[ \sum_{t=1}^T r(s_t, a_t) \right] = \arg\max_{\theta \in \Theta} \ \mathbb{E}_{\tau \sim \rho_\theta(\tau)} \left[ r(\tau) \right]
$$

>Note : $\rho_\theta(\tau) = P(s_1)\prod_{t=1}^{T} \pi_\theta(a_t \mid s_t) \cdot P(s_{t+1} \mid s_t, a_t)$

Now that we have some notion about the optimality of a policy we need to figure out how to update $\theta$ so that we reach $\theta^\ast$. As we have mentioned before that we'll use Gradient Ascent for making the updates but have not quite justified it. The reason for making the updates in the direction of the ascent (and not descent) is simply due to our maximization objective i.e *We need to follow the slope to find the peak*. And secondly, we need to figure out the most important part, the Gradient !



## âœï¸ Calculating the Gradient

Let's calculate the gradient of the policy $\pi_\theta$:

$$
\begin{align*}
\nabla_\theta\eta(\pi_\theta)=\nabla_\theta \mathbb{E}_{\tau \sim \rho_\theta(\tau)} \left[ \sum_{t=1}^T r(s_t, a_t) \right] &= \nabla_\theta \int \rho_\theta(\tau)\cdot r(\tau) \,d\tau & r(\tau) =  \sum_{t=1}^T r(s_t, a_t)\\
&= \int \nabla_\theta \rho_\theta(\tau)\cdot r(\tau) \,d\tau \\
&= \int \rho_\theta(\tau) \frac{\nabla_\theta \rho_\theta(\tau)}{\rho_\theta(\tau)} \cdot r(\tau) \,d\tau \\
&= \int \nabla_\theta \log \rho_\theta(\tau)\cdot r(\tau) \,d\tau \\
&= \mathbb{E}_{\tau \sim \rho_\theta(\tau)} \left[ \nabla_\theta \log \rho_\theta(\tau)\cdot r(\tau) \right]
\end{align*}
$$

Let's unroll $\rho_\theta(\tau)$ and calculate its derivative:

$$
\begin{align*}
\nabla_\theta \log \rho_\theta(\tau)&=\nabla_\theta \log \left[P(s_1)\prod_{t=1}^{T} \pi_\theta(a_t \mid s_t) \cdot P(s_{t+1} \mid s_t, a_t)\right]\\
&=\nabla_\theta \left[\log P(s_1)+\sum_{t=1}^{T} \log\pi_\theta(a_t \mid s_t) +\sum_{t=1}^{T} \log P(s_{t+1} \mid s_t, a_t)\right]\\
&=\nabla_\theta\left[\sum_{t=1}^T\log\pi_\theta(a_t \mid s_t) \right]
\end{align*}
$$

Substituting the value we get 

$$
\begin{align*}
\nabla_\theta \eta(\pi_\theta)
&= \mathbb{E}_{\tau \sim \rho_\theta(\tau)} \left[ \nabla_\theta\left(\sum_{t=1}^T\log\pi_\theta(a_t \mid s_t) \right)r(\tau) \right]\\
&=\mathbb{E}_{\tau \sim \rho_\theta(\tau)} \left[\left(\sum_{t=1}^T \nabla_\theta\log\pi_\theta(a_t \mid s_t) \right) \left(\sum_{t=1}^T r(s_t, a_t)\right) \right]\\
&\approx\frac{1}{m}\sum_{i=1}^m\left[\left(\sum_{t=1}^T \nabla_\theta\log\pi_\theta(a_t^i \mid s_t^i) \right) \left(\sum_{t=1}^T r(s_t^i, a_t^i)\right) \right]
\end{align*}
$$

> Note : In practice the empirical estimate is used which is practiced by averaging over $m$ trajectories and $s_t^i, a_t^i$ represents the state and action pair at time step $t$ in trajectory $i$

So there we have it, the gradient of the performance of the policy $\pi_\theta$. How do we implement this algorithm ? 
- Run the policy $\pi_\theta$
- Calculate $\nabla_\theta \eta(\pi_\theta)$
- Update $\theta \leftarrow \theta + \alpha \nabla_\theta \eta(\pi_\theta)$

Well, we've got it the gradient of the policy, but let's now look at another way of deriving the policy gradient. Till now, we haven't used much of the RL terminologies but in order to truly appreciate the policy gradient, we require some background. The reader can check out this execellent blog [^2] by Lilian Weng to grasp the background on some of the fundamental concepts used in RL.

## Policy gradient from the lens of Bellman

I am assuming that you all have checked out [^2], so with this assumption let's proceed. Now let's imagine we have a policy $\pi_\theta$ and let's say at state $s_t$, most of the time action $a_i$ is executed by $\pi_\theta$. But if we want to find out what if we execute $a_j : a_j\neq a_i$ this one time when we come to $s_t$ and for the rest of the time we behave according to $\pi_\theta$ We take the help of $Q$ values.

$$
Q_{\pi_\theta}(s_t, a_j) = \mathbb{E}_{(s_{t+1}, a_{t+1}, \cdots, s_T, a_T) \sim \rho_\theta(\tau|s_t, a_j)}\left[\sum_{k=1}^Tr_{t+k}\right]
$$

>We're getting rid of $r(s_t, a_t)$ and instead we'll now use $r_{t+1}$ to denote the reward for transitioning from $s_t$ to $s_{t+1}$ by executing $a_t$. And same as before, we assume $\gamma = 1$

In other words, $Q_{\pi_\theta}(s_t, a_j)$ denotes the expected return obtained for executing $a_j$ in $s_j$ and then behaving according to $\pi_\theta$ for the rest of the time. In a sense it evaluates a state-action pair under a given policy. Now what if we want to find out how good a state (not the state-action pair) is ? I mean it only seems logical to execute an action in a state if it's good enough. For this we require the value functions. 

$$
\begin{align*}
V_{\pi_\theta}(s_t) &= \mathbb{E}_{( a_t, s_{t+1}, a_{t+1}, \cdots, s_T, a_T) \sim \rho_\theta(\tau|s_t)}\left[\sum_{k=1}^Tr_{t+k}\right]\\
&=\mathbb{E}_{a_t \sim \pi_\theta(.\mid s_t) }\left[\mathbb{E}_{(s_{t+1}, a_{t+1}, \cdots, s_T, a_T) \sim \rho_\theta(\tau|s_t, a_t)}\left[\sum_{k=1}^Tr_{t+k}\right]\right]\\
&=\mathbb{E}_{a_t \sim \pi_\theta(.\mid s_t) }\left[Q_{\pi_\theta}(s_t, a_t)\right]\\
& = \sum_{a_t}\pi_\theta(a_t|s_t)Q_{\pi_\theta}(s_t, a_t)
\end{align*}
$$

We can also write the Q value function with the help of Value function :

$$
\begin{align*}
Q_{\pi_\theta}(s_t, a_t) &= \mathbb{E}_{(s_{t+1}, a_{t+1}, \cdots, s_T, a_T) \sim \rho_\theta(\tau|s_t, a_t)}\left[\sum_{k=1}^Tr_{t+k}\right]\\
&=\mathbb{E}_{s_{t+1}\sim P(s_{t+1}|s_t, a_t)}\left[r_{t+1} + \mathbb{E}_{(a_{t+1}, \cdots, s_T, a_T) \sim \rho_\theta(\tau|s_{t+1})}\left[\sum_{k=1}^Tr_{t+1+k}\right]\right]\\
&=\sum_{s_{t+1}, r_{t+1}} P(s_{t+1}, r_{t+1}\mid s_t, a_t)\cdot r_{t+1} + \sum_{s_{t+1}} P(s_{t+1}\mid s_t, a_t) V_{\pi_\theta}(s_{t+1})
\\
&=\sum_{s_{t+1}, r_{t+1}} P(s_{t+1}, r_{t+1}\mid s_t, a_t)\left[ r_{t+1} +  V_{\pi_\theta}(s_{t+1})\right]
\end{align*}
$$

So Value function of a state essentially quantifies the goodness of a state in terms of the expected return which can be obtained from the state under a certain policy. Therefore a "good" policy will make the agent behave in a manner so that it encounters "good" states. Thus we can rewrite the perfomance of a policy as 

$$
\eta(\pi_\theta)=\eta(\theta) = \mathbb{E}_{s\in S}\left[V_{\pi_\theta}(s)\right] 
$$

Just to simplify our objective a bit, we assume that there the starting state $s_0$ is  non-random (fixed) and this is okay. If this state is a probable start state or in most games where the starting state is pretty much fixed, this assumption perefectly preserves consistency in our optimization objective.

Therefore with the new assumption, the objective now becomes :

$$
\theta^* = \arg \max_{\theta \in \Theta}\eta(\theta) = \arg \max_{\theta \in \Theta}V_{\pi_\theta}(s_0)=\arg \max_{\theta \in \Theta}\sum_{a} \pi_\theta(a\mid s_0)Q_{\pi_\theta}(s_0)
$$

I am not sure if you can spot something here but it will become obvious in a minute, that the gradient now depends on both the policy and the Q function.
Let's derive the gradient now 

$$
\begin{align*}
\nabla_{\theta}\eta(\theta) &= \nabla_{\theta}V_{\pi_{\theta}}(s) \\
&= \sum_{a} \bigl( Q_{\pi_{\theta}}(s, a) \nabla_{\theta} \pi_{\theta}(a|s) + \pi_{\theta}(a|s) \nabla_{\theta} Q_{\pi_{\theta}}(s, a) \bigr) \\
&= \sum_{a} \bigl( Q_{\pi_{\theta}}(s, a) \nabla_{\theta} \pi_{\theta}(a|s) + \pi_{\theta}(a|s) \nabla_{\theta} \sum_{s', r} P(s', r \mid s, a)\bigl[r + V_{\pi_\theta}(s')\bigr] \bigr) \\
&= \sum_{a} \bigl( Q_{\pi_{\theta}}(s, a) \nabla_{\theta} \pi_{\theta}(a|s) + \pi_{\theta}(a|s) \sum_{s'} P(s' \mid s, a) \nabla_{\theta} V_{\pi_\theta}(s') \bigr) \\
&\quad \text{\footnotesize$\bigl\{ \because \sum_{s', r} P(s', r \mid s, a) \nabla_{\theta} r = 0 \bigr\}$}
\end{align*}
$$

Let $\Delta(s) = \sum_{a}Q_{\pi_{\theta}}(s, a)\nabla_{\theta}\pi_{\theta}(a|s)$

Hence , we have : 
$$
\begin{align*}
    
  \nabla_{\theta}V_{\pi_{\theta}}(s) = \Delta(s) + \sum_{a}\pi_{\theta}(a|s)\sum_{s'}P(s'|s, a)\nabla_{\theta}V_{\pi_\theta}(s')

\end{align*}
$$
Using this recursive formulation, we can unroll the equation as : 
$$
\begin{align*}
    
  \nabla_{\theta}V_{\pi_{\theta}}(s) = \Delta(s) + \sum_{a}\pi_{\theta}(a|s)\sum_{s'}P(s'|s, a)\bigl(\Delta(s') + \sum_{a'}\pi_{\theta}(a'|s')\sum_{s''}P(s''|s', a')\nabla_{\theta}V_{\pi_\theta}(s'')\bigr)

\end{align*}
$$

Before we proceed further, let's take a detour. Let's formalize the probability of visiting a state $x$ from state $s$ in $k$ steps under a policy $\pi$ as : $\phi_\pi (s\rightarrow x, k)$.


  - $\phi_\pi (s\rightarrow s, 0) = 1 $
  - $\phi_\pi (s\rightarrow x, 1) = \sum_{a}\pi(a|s)P(x|s, a) $
  - $\phi_\pi (s\rightarrow x, 2) = \sum_{a}\pi(a|s)\sum_{s'}P(s'|s, a)\sum_{a'}\pi(a'|s')P(x|s', a')$
  - $\phi_\pi (s\rightarrow x, 2) = \sum_{s'}\phi_\pi (s\rightarrow s', 1)\phi_\pi (s'\rightarrow x, 1)$
  -  $\phi_\pi (s\rightarrow x, 3) = \sum_{a}\pi(a|s)\sum_{s'}P(s'|s, a)\sum_{a'}\pi(a'|s')\sum_{s''}P(s''|s', a')\sum_{a''}\pi(a''|s'')P(x|s'', a'')$ 
  - $\phi_\pi (s\rightarrow x, 3) = \sum_{s''}\phi_\pi (s\rightarrow s'', 2)\phi_\pi (s''\rightarrow x, 1)$
  -  $\phi_\pi (s\rightarrow x, k+1) = \sum_{s'}\phi_\pi (s\rightarrow s', k)\phi_\pi (s'\rightarrow x, 1)$ 

Coming back to the proof:

$$
  \begin{align*}

  \nabla_{\theta}V_{\pi_{\theta}}(s) &= \Delta(s) + \sum_{a}\pi_{\theta}(a|s)\sum_{s'}P(s'|s, a)\bigg(\Delta(s') + \sum_{a'}\pi_{\theta}(a'|s')\sum_{s''}P(s''|s', a')\nabla_{\theta}V_{\pi_\theta}(s'')\bigg)\\
  &= \Delta(s) + \sum_{s'}\phi_{\pi_\theta}(s\rightarrow s', 1)\Delta(s') + \sum_{s''}\phi_{\pi_\theta}(s\rightarrow s'', 2)\nabla_{\theta}V_{\pi_\theta}(s'')\\
  &= \Delta(s) + \sum_{s'}\phi_{\pi_\theta}(s\rightarrow s', 1)\Delta(s') + \sum_{s''}\phi_{\pi_\theta}(s\rightarrow s'', 2)\Delta(s'') + \cdots \infty\\
  &= \phi_{\pi_\theta}(s\rightarrow s, 0)\Delta(s) + \sum_{s'}\phi_{\pi_\theta}(s\rightarrow s', 1)\Delta(s') + \sum_{s''}\phi_{\pi_\theta}(s\rightarrow s'', 2)\Delta(s'') + \cdots \infty\\
  &= \sum_{x\in S}\sum_{k=0}^\infty\phi_{\pi_\theta}(s\rightarrow x, k)\Delta(x)

\end{align*}
$$

Let $n(s)$ denote the number of time steps spent on average in state $s$ in a single episode. Formally, 
$$

  \begin{align*}
        
            n(s) &= h(s)+\sum_{s'}n(s')\sum_{a}\pi_\theta(a|s')P(s|s',a)\\
    &=h(s)+\sum_{s'}n(s')\phi_{\pi_\theta}(s'\rightarrow s, 1)
        
  \end{align*}    

$$

$h(s)$ denotes the probability that the episode begins in a state $s$

$$
    \begin{align*}
        
            n(s)&=h(s)+\sum_{s'}n(s')\phi_{\pi_\theta}(s'\rightarrow s, 1)\\
            &= h(s)+\sum_{s'}\phi_{\pi_\theta}(s'\rightarrow s, 1)\bigg(h(s')+\sum_{s''}n(s'')\phi_{\pi_\theta}(s''\rightarrow s', 1)\bigg)\\
            &= h(s)+\sum_{s'}\phi_{\pi_\theta}(s'\rightarrow s, 1)h(s')+\sum_{s''}\phi_{\pi_\theta}(s''\rightarrow s, 2)n(s'')\\
             &= h(s)+\sum_{s'}\phi_{\pi_\theta}(s'\rightarrow s, 1)h(s')+\sum_{s''}\phi_{\pi_\theta}(s''\rightarrow s, 2)h(s'')+\cdots\infty\\
              &= \sum_{s'}\sum_{k=0}^\infty \phi_{\pi_\theta}(s'\rightarrow s, k)h(s')
        
    \end{align*}    
$$

Since we assumed that the episodes start from a particular non random state $s$, we have therefore :
$$
    \begin{align*}
        
            h(s) = \begin{cases}
                1 &\text{if }s=s\\
                0 &\text{if }s\neq s
            \end{cases}
      
    \end{align*}
$$

Thus, $n(s)$ reduces to :
$$
    \begin{align*}
        \begin{split}
            n(s)&= \sum_{s'}\sum_{k=0}^\infty \phi_{\pi_\theta}(s'\rightarrow s, k)h(s')\\
            &=\sum_{k=0}^\infty \phi_{\pi_\theta}(s\rightarrow s, k)h(s)\\
            &=\sum_{k=0}^\infty \phi_{\pi_\theta}(s\rightarrow s, k)
        \end{split}
    \end{align*}    
$$

Coming back to our policy gradient proof, we can now rewrite the equation as :

$$

    \begin{align*}
  \nabla_{\theta}V_{\pi_{\theta}}(s) &= \sum_{x\in S}\sum_{k=0}^\infty\phi_{\pi_\theta}(s\rightarrow x, k)\Delta(x)\\
  &=\sum_{x\in S}n(x)\Delta(x)\\
  &=\sum_{x\in S}n(x)\sum_{a}Q_{\pi_{\theta}}(x, a)\nabla_{\theta}\pi_{\theta}(a|x)\\
  &=\bigg(\sum_{x'\in S}n(x')\bigg)\sum_{x\in S}\frac{n(x)}{\sum_{x'\in S}n(x')}\sum_{a}Q_{\pi_{\theta}}(x, a)\nabla_{\theta}\pi_{\theta}(a|x)\\
  &\propto\sum_{x\in S}\frac{n(x)}{\sum_{x'\in S}n(x')}\sum_{a}Q_{\pi_{\theta}}(x, a)\nabla_{\theta}\pi_{\theta}(a|x)\\
  &=\sum_{x\in S}\mu_{\pi_{\theta}}(x)\sum_{a}Q_{\pi_{\theta}}(x, a)\nabla_{\theta}\pi_{\theta}(a|x)
    \end{align*}

$$

>Note: that the reason for the spot division is because of the possibility of the probability values being greater than one (for a fixed start environment $n(x) > 1$), so we had to normalize it. $\mu_{\pi_{\theta}}$ is the on-policy distribution (the fraction of time spent in each state) under policy $\pi_\theta$. 

So, the question is are both the derivations same ? the asnwer is Yes ! With a bit of work, we can show that both these equations are infact the same

$$
\begin{align*}
\nabla_\theta \eta(\pi_\theta)
&= \mathbb{E}_{\tau \sim \rho_\theta(\tau)} \left[ \nabla_\theta\left(\sum_{t=1}^T\log\pi_\theta(a_t \mid s_t) \right)r(\tau) \right]\\
&=  \mathbb{E}_{\tau \sim \rho_\theta(\tau)}\left[ \nabla_\theta\left(\sum_{t=1}^T\log\pi_\theta(a_t \mid s_t) \left(\sum_{t'=1}^Tr(s_{t'}, a_{t'})\right)\right) \right]\\
&=  \mathbb{E}_{\tau \sim \rho_\theta(\tau)}\left[ \nabla_\theta\left(\sum_{t=1}^T\log\pi_\theta(a_t \mid s_t) \left(\sum_{t'=1}^{t-1}r(s_{t'}, a_{t'}) 
+ \sum_{t'=t}^{T}r(s_{t'}, a_{t'})\right)\right) \right]\\
& \text{integrating the causality trick}\\
&\approx  \mathbb{E}_{\tau \sim \rho_\theta(\tau)}\left[ \nabla_\theta\left(\sum_{t=1}^T\log\pi_\theta(a_t \mid s_t) \left(\sum_{t'=t}^{T}r(s_{t'}, a_{t'})\right)\right) \right] \ \ \ \ \because \pi_\theta(a_t|s_t) \perp  \sum_{t'=1}^{t-1}r(s_{t'}, a_{t'}) \\
&\text{replacing } \sum_{t'=t}^{T}r(s_{t'}, a_{t'}) \text{ by } Q_{\pi_\theta}(s_t, a_t) \because Q_{\pi_\theta}(s_t, a_t) = \mathbb{E}_{s_{t+1}, a_{t+1}, \cdots \sim \rho_\theta(\tau\mid s_t, a_t)}\left[\sum_{t'=t}^{T}r(s_{t'}, a_{t'})\right]\\
&= \mathbb{E}_{\tau \sim \rho_\theta(\tau)}\left[ \nabla_\theta\left(\sum_{t=1}^T \log\pi_\theta(a_t \mid s_t)\right)Q_{\pi_\theta}(s_t, a_t) \right] \\
&= \sum_{t=1}^T \mathbb{E}_{(s_t, a_t) \sim P_\theta^t(s_t, a_t)} \left[ \nabla_\theta\log\pi_\theta(a_t \mid s_t)Q_{\pi_\theta}(s_t, a_t)  \right]
\end{align*}
$$

$P^t_{\theta}(s, a)$ is the state-action joint distribution at time $t$. We can also write it as $P^t_{\theta}(s, a) = \mu_\theta^t(s)\pi_\theta(a\mid s)$ where $\mu_\theta^t$ is the on-policy distribution at time $t$. Now integrating all the timesteps we can achieve the steady state state-action marginal $P_\theta(s, a) = \mu_\theta(s)\pi_\theta(a\mid s)$. Thus, continuing from before 

$$
\begin{align*}
\nabla_\theta \eta(\pi_\theta)&=\sum_{t=1}^T \mathbb{E}_{(s_t, a_t) \sim P_\theta^t(s_t, a_t)} \left[ \nabla_\theta\log\pi_\theta(a_t \mid s_t)Q_{\pi_\theta}(s_t, a_t)  \right]\\
&=\mathbb{E}_{(s, a) \sim P_\theta(s, a)} \left[ \nabla_\theta\log\pi_\theta(a \mid s)Q_{\pi_\theta}(s, a)  \right]\\
&=\sum_{s, a }\mu_\theta(s)\pi_\theta(a\mid s) \nabla_\theta\log\pi_\theta(a \mid s)Q_{\pi_\theta}(s, a)  \\
&=\sum_{s, a }\mu_\theta(s) \nabla_\theta\pi_\theta(a \mid s)Q_{\pi_\theta}(s, a)\\
&=\sum_{s}\mu_\theta(s) \sum_a \nabla_\theta\pi_\theta(a \mid s)Q_{\pi_\theta}(s, a)\\
\end{align*}
$$


## References

[^1]: [Sergey Levine RAIL lectures from UC Berkeley](https://rail.eecs.berkeley.edu/deeprlcourse/).
[^2]:[A long peek into Reinforcement Learning](https://lilianweng.github.io/posts/2018-02-19-rl-overview/)

