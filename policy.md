<script type="text/javascript">
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      renderActions: {
        findScript: [10, function (doc) {
          // escape Markdown italics (*) inside math
          for (const script of document.querySelectorAll('script[type^="math/tex"]')) {
            script.text = script.text.replace(/\^([^\s^_{}\\])/g, '^{$1}');
          }
        }, '']
      }
    }
  };
</script>

<script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


# 🎯 Policy Gradient: *L’Equazione Elegante*

In this blog, I aim to explore the elegant derivation of the policy gradient. Why? Because I find the equation not just insightful, but genuinely beautiful in its simplicity and structure. And I’d like to share that appreciation with anyone who reads this.

So, the goal here is to explain the derivation in a way that’s digestible for a novice reader. That said, there might be parts where I don’t explain things well enough — and that could interrupt the flow a bit. For that, I ask a little patience (and effort) from you, the reader. Honestly, this is also my way of pushing myself to understand things better to the point where I can explain them without using fancy terminology at all. But hey, this is just *iteration one*, and I’ll keep improving it as I go. So, pardon me — and thanks in advance for bearing with me. 

>These notes are based from the lectures of Sergey Levine [^1], Lilian Wang blog and the Reinforcement Learning book by Sutton and Barto

---

So first of all what's a policy ? A policy $\pi$ is simply an agent's way of behaving in an environment. More specifically, if we consider $A$ = {$a_1, \cdots, a_k$ } to be the set of actions applicable in a particular state $s_t$ of an environment (at timestep $t$), then the policy specifies a probability distribution over these $k$ actions. 

$$
\pi(a_t=a_i|s_t)=P(a_i|s_t) \ \ \ \ i\in\{1, \cdots, k\}
$$

Now, the question is how do we find that optimal policy ? More specifically what makes a policy optimal ? These are some of the questions we need to answer. 

Since we are introducing terms in the line of optimality, we need to first quantify the poilicies. A viable method would be to assess the performance of a policy by finding out the expected return obtained from behaving according to the policy in a particular environment. In other words, the performance of a policy $\eta(\pi)$ can be assessed by running the policy multiple times (ideally infinite) and summing the rewards obtained at each time step $r(s_t, a_t)$ until termination. 

Let $\tau = (s_1, a_1, \cdots, s_T, a_T)$ be a sample trajectory generated using $\pi$. Thus the probability of this particular trajectory being generated by $\pi$ is 
$$
\rho_\pi(\tau) = P(s_1)\prod_{t=1}^{T} \pi(a_t \mid s_t) \cdot P(s_{t+1} \mid s_t, a_t)
$$
> Note : The environment is markov i.e $P(s_{t+1}\mid s_t, a_t)=P(s_{t+1}\mid s_0, a_0, \cdots, s_t, a_t)$

Therefore, $\eta(\pi)$ can be formulated as :
$$
\eta(\pi) = \mathbb{E}_{\tau \sim \rho_\pi(\tau)}\left[\sum_{t=1}^Tr(s_t, a_t)\right]
$$
> Note : Generally we discount the rewards in the form of $\sum_{t=1}^T\gamma^{t-1} r(s_t, a_t)$, and here we have kept $\gamma=1$

With this, we now have a method to quantify the performance of a policy $\pi$. Now what makes a policy optimal? If $\Pi$={ $\pi_1, \pi_2, \cdots, \pi_\infty $ } is the space of all policies, the optimal policy $\pi^\ast$ is the one for which the expected return is the highest. In other words,

$$
\pi^\ast = \arg \max_{\pi\sim \Pi} \eta(\pi)
$$

But $\Pi$ is continous and needless to point out that noones's willing to check each policy out one by one until the end of time when still infinitely more  policies will be left unchecked. So, what do we do ? Well we can parametrize the policy with $\theta$ i.e we can model the policy as a neural network $f$ the parameters of which are $\theta$ and then use *Gradient Ascent* to update the parameters. Also now our search space is no more $\Pi$, as some policies might never get modelled by the network. Therefore our search space is now $\tilde{\Pi}\subseteq \Pi$ where $\tilde{\Pi}$ = { $\pi_\theta\mid \pi_\theta = f(\theta) , \ \ \ \pi_\theta \in \Pi, \ \ \ \forall \theta \in \Theta $ }. By reducing the search space, we can't gurantee global optimality anymore, however its better than waiting till the end of time and that too for solving one environment. Also, our new modified objective is formulated as :

$$
\theta^* = \arg\max_{\theta \in \Theta} \ \mathbb{E}_{\tau \sim \rho_\theta(\tau)} \left[ \sum_{t=1}^T r(s_t, a_t) \right] = \arg\max_{\theta \in \Theta} \ \mathbb{E}_{\tau \sim \rho_\theta(\tau)} \left[ r(\tau) \right]
$$

>Note : $\rho_\theta(\tau) = P(s_1)\prod_{t=1}^{T} \pi_\theta(a_t \mid s_t) \cdot P(s_{t+1} \mid s_t, a_t)$

Now that we have some notion about the optimality of a policy we need to figure out how to update $\theta$ so that we reach $\theta^\ast$. As we have mentioned before that we'll use Gradient Ascent for making the updates but have not quite justified it. The reason for making the updates in the direction of the ascent (and not descent) is simply due to our maximization objective i.e *We need to follow the slope to find the peak*. And secondly, we need to figure out the most important part, the Gradient !



## ✏️ Calculating the Gradient

Let's calculate the gradient of the policy $\pi_\theta$:

$$
\begin{align*}
\nabla_\theta\eta(\pi_\theta)=\nabla_\theta \mathbb{E}_{\tau \sim \rho_\theta(\tau)} \left[ \sum_{t=1}^T r(s_t, a_t) \right] &= \nabla_\theta \int \rho_\theta(\tau)\cdot r(\tau) \,d\tau & r(\tau) =  \sum_{t=1}^T r(s_t, a_t)\\
&= \int \nabla_\theta \rho_\theta(\tau)\cdot r(\tau) \,d\tau \\
&= \int \rho_\theta(\tau) \frac{\nabla_\theta \rho_\theta(\tau)}{\rho_\theta(\tau)} \cdot r(\tau) \,d\tau \\
&= \int \nabla_\theta \log \rho_\theta(\tau)\cdot r(\tau) \,d\tau \\
&= \mathbb{E}_{\tau \sim \rho_\theta(\tau)} \left[ \nabla_\theta \log \rho_\theta(\tau)\cdot r(\tau) \right]
\end{align*}
$$

Let's unroll $\rho_\theta(\tau)$ and calculate its derivative:

$$
\begin{align*}
\nabla_\theta \log \rho_\theta(\tau)&=\nabla_\theta \log \left[P(s_1)\prod_{t=1}^{T} \pi_\theta(a_t \mid s_t) \cdot P(s_{t+1} \mid s_t, a_t)\right]\\
&=\nabla_\theta \left[\log P(s_1)+\sum_{t=1}^{T} \log\pi_\theta(a_t \mid s_t) +\sum_{t=1}^{T} \log P(s_{t+1} \mid s_t, a_t)\right]\\
&=\nabla_\theta\left[\sum_{t=1}^T\log\pi_\theta(a_t \mid s_t) \right]
\end{align*}
$$

Substituting the value we get 

$$
\begin{align*}
\nabla_\theta \eta(\pi_\theta)
&= \mathbb{E}_{\tau \sim \rho_\theta(\tau)} \left[ \nabla_\theta\left(\sum_{t=1}^T\log\pi_\theta(a_t \mid s_t) \right)r(\tau) \right]\\
&=\mathbb{E}_{\tau \sim \rho_\theta(\tau)} \left[\left(\sum_{t=1}^T \nabla_\theta\log\pi_\theta(a_t \mid s_t) \right) \left(\sum_{t=1}^T r(s_t, a_t)\right) \right]\\
&\approx\frac{1}{m}\sum_{i=1}^m\left[\left(\sum_{t=1}^T \nabla_\theta\log\pi_\theta(a_t^i \mid s_t^i) \right) \left(\sum_{t=1}^T r(s_t^i, a_t^i)\right) \right]
\end{align*}
$$

> Note : In practice the empirical estimate is used which is practiced by averaging over $m$ trajectories and $s_t^i, a_t^i$ represents the state and action pair at time step $t$ in trajectory $i$

So there we have it, the gradient of the performance of the policy $\pi_\theta$. How do we implement this algorithm ? 
- Run the policy $\pi_\theta$
- Calculate $\nabla_\theta \eta(\pi_\theta)$
- Update $\theta \leftarrow \theta + \alpha \nabla_\theta \eta(\pi_\theta)$

Well, we've got it the gradient of the policy, but let's now look at another way of deriving the policy gradient. Till now, we haven't used much of the RL terminologies but in order to truly appreciate the policy gradient, we require some background. The reader can check out this execellent blog [^2] by Lilian Weng to grasp the background on some of the fundamental concepts used in RL. 

## References

[^1]: [Sergey Levine RAIL lectures from UC Berkeley](https://rail.eecs.berkeley.edu/deeprlcourse/).
[^2]:[A long peek into Reinforcement Learning](https://lilianweng.github.io/posts/2018-02-19-rl-overview/)

